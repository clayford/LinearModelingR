---
title: "Linear Modeling in R"
author: "Clay Ford, Statistical Research Consultant, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## Quick Intro to R Notebooks and R Markdown

This is an R Markdown Notebook. It combines markdown, a plain text formatting syntax, and R code. When you execute R code within the notebook, the output appears beneath the code. 

This file was created in RStudio by going to File...New File...R Notebook.

R code needs to be in "chunks" for it to work. Below is an example of an R code chunk. It makes a parabola using base R graphics.

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
x <- seq(-1,1,by = 0.01)
y <- x^2
plot(x, y, type = "l")
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## Exercise 0 (code along)

Insert a new R code chunk below and type/run the code: 22/7



## Linear Modeling with Simulated Data

Instead of using theory and formulas, let's explore linear modeling using simulated data. 

Below we assign to `x` the values 1 - 25. Then we generate `y` as a function of `x`:

```{r}
x <- 1:25
y <- 10 + 5*x  # y = mx + b
d <- data.frame(x, y)
plot(y ~ x, data = d)
```


10 is the intercept, 5 is the slope. `y` is completely determined by `x`.

Now let's add some "noise" to our data by adding random draws from a Normal
distribution with mean = 0 and a standard deviation = 10.

`set.seed(1)` ensures we all generate the same "random" data:

```{r}
set.seed(1)
noise <- rnorm(n = 25, mean = 0, sd = 10)
# Add the noise to 10 + 5*x and re-draw plot
d$y <- 10 + 5*x + noise
plot(y ~ x, data = d)
```

This data is the combination of two parts:

1. 10 + 5*x
2. rnorm(n = 25, mean = 0, sd = 10)

What if we were given this data and told to determine the process that generated it? In other words, fill in the blanks:

1. ___________
2. rnorm(n = 25, mean = 0, sd = ____)

That's basically what linear modeling/regression is.

Traditional linear modeling assumes the following (among others):

1. the formula is a weighted sum of predictors (eg, 10 + 5*x)
2. the noise is a random draw from a Normal distribution with mean = 0
3. the standard deviation of the Normal distribution is constant

Linear modeling tries to recover the weights in the first assumption (10 and
5) and the standard deviation in the 3rd assumption (10).

Let's attempt to recover the data generating process. For this we use the `lm()` function. We have to specify the formula for the first assumption. The 2nd and 3rd assumptions are built into `lm()`.

"y ~ x" means we think Part 1 of the model is "y = intercept + slope*x". This tells `lm()` to take our data and find the best intercept and slope. Notice this is the correct model! 

You'll usually want to save the model to an object. Below I save it to "mod". Then we view the results of the model using `summary()`

```{r}
mod <- lm(y ~ x, data = d)
summary(mod)
```


The model returns the following estimates:

- intercept = 11.135
- slope = 5.042
- sd = 9.7 (Residual standard error)

This says every one-unit increase in `x` increases `y` by about 5, with uncertainty of about 9.7. These are pretty close to the "true" values of 10, 5, and 10 we used to generate the data.  

**In real life, we DO NOT KNOW the formula of weighted sums, or even if a formula of weighted sums is appropriate. We also don't know if the Normality assumption or constant variance assumption of the noise is plausible.**

What can we do with the fitted model?

We could use those estimates to generate data and see if they look similar to our original data. (Run entire chunk at once.)

```{r}
y2 <- 11.135 + 5.042*x + rnorm(25, 0, 9.7)
plot(y ~ x, data = d)
points(x, y2, col = "red")
```

We could also add the original and fitted lines. The red line is the fitted line. Given the data, this is what `lm()` determined was the "best fitting" line, or the model that generated the data. (Run entire chunk at once.)

```{r}
plot(y ~ x, data = d)
abline(a = 10, b = 5) # a = intercept, b = slope
abline(mod, col = "red") # a = 11.135, b = 5.042
```

We could also compare smooth density curves of the original and model-generated data. Smooth density curves are basically smooth versions of histograms. Data generated by our model should have a similar distribution to the original data. This looks great. (Run entire chunk at once.)

```{r}
plot(density(d$y))
lines(density(y2), col = "red")
```

We can use this model to make a prediction using the `predict()` function. The `interval = "confidence"` arguments says return a 95% CI. 

```{r}
predict(mod, newdata = data.frame(x = 10), interval = "confidence")
```

The expected mean of y when x = 10 is about 61.6 with a CI of (57.2, 65.9).

**This is linear modeling:**

1. propose and fit model(s)
2. determine if the model is good
3. use the model to explain relationships or make predictions


## YOUR TURN #1 

Submit the following code to simulate some data:

```{r}
set.seed(2)
x1 <- sample(1:5, size = 1000, replace = TRUE, 
             prob = c(0.1,0.2,0.3,0.3,0.1))
x2 <- rnorm(n = 1000, mean = 12, sd = 2)
noise <- rnorm(n = 1000, mean = 0, sd = 4)
y <- 5 + 10*x1 + -4*x2 + noise
df <- data.frame(y, x1, x2)
head(df)
```

Insert a code chunk below and use `lm()` to attempt to recover the "true" values of 5, 10, -4 and 4. In other words:

- model y as a function of x1 and x2, OR
- regress y on x1 and x2

Hint: use a plus (+) sign to include multiple predictors in a model.

```{r}
mod2 <- lm(y ~ x1 + x2, data = df)
summary(mod2)
```


## Import data

Let's import the data we'll be using today. The data we'll work with is Albemarle County real estate data which was downloaded from the Office of Geographic Data Services. We'll use a random sample of the data.

```{r}
URL <- 'https://raw.githubusercontent.com/clayford/dataviz_with_ggplot2/master/alb_homes.csv'
homes <- read.csv(file = URL)
```

Let's look at the first few rows:

```{r}
head(homes)
```

Variable name definitions:

- *yearbuilt*: year house was built
- *finsqft*: size of house in number square feet
- *cooling*: 'Central Air' versus 'No Central Air'
- *bedroom*: number of bedrooms
- *fullbath*: number of full bathrooms (toilet, sink and bath)
- *halfbath*: number of half bathrooms (toilet and sink only)
- *lotsize*: size of land on which home is located, in acres
- *totalvalue*: total assessed value of home and property
- *esdistrict*: the elementary school the home feeds into
- *msdistrict*: the middle school the home feeds into
- *hsdistrict*: the high school the home feeds into
- *censustract*: the census tract the home is located in
- *age*: of the house in years as of 2018
- *condition*: assessed condition of home (Substandard, Poor, Fair, Average, Good, Excellent)
- *fp*: indicator if house has fireplace (0=no, 1=yes)


## Linear Modeling with Real Estate Data

Let's say we want to model the mean total value of a home as a function of various characteristics such as lot size, finished square feet, presence of central air, etc.

We have to propose a linear model. Unlike the previous example this is not simulated data for which we know the data generating process. How to propose a model? It really helps to have some subject knowledge expertise. 

Let's fit a linear model using finsqft, bedrooms and lotsize. The plus (+) sign means "include" in model.

```{r}
m1 <- lm(totalvalue ~ finsqft + bedroom + lotsize, data = homes)
summary(m1)

```


The coef() function extracts the coefficients (or weights):

```{r}
coef(m1)
```

Some naive interpretation:

- each additional finished square foot adds about $284 to price
- each additional bedroom drops the price by $13,218 (?)
- each additional lot size acre adds about $4268 to price

Remember, the interpretation assumes all other variables are held constant. So adding a bedroom to a house, without increasing the lot size or finished square feet of the house, is estimated to drop the value of a home. Does this make sense?

Is this a "good" model? There are many statistical metrics to help evaluate this, such as R-squared. But let's simulate data from the model and compare it to our observed data. A "good" model should generate data that looks similar to the original data. 

We could do this by hand:

```{r}
sim_value <- -133328.2482 + 284.4613*homes$finsqft + 
  -13218.4091*homes$bedroom + 4268.7655*homes$lotsize + 
  rnorm(3025, sd = 227200)
```

An easier and faster way is to use the `simulate` function which allows you to generate multiple samples. Here we generate 50 samples. Each sample will have the same number of observations as our original sample (n = 3025). Each sample value is generated using our observed values for `finsqft`, `bedroom`, and `lotsize`.

```{r}
sim1 <- simulate(m1, nsim = 50)
```


Now let's plot our simulated data with our observed data using smooth density plots. (Run entire chunk at once.)

```{r}
plot(density(homes$totalvalue))
for(i in 1:50)lines(density(sim1[[i]]), col = "grey80")

```

This does not appear to be a great model. In fact some of our simulated values are negative!

Recall our main assumptions:

1) totalvalue is a weighted sum: 
   totalvalue = Intercept + finsqft + bedrooms + lotsize
2) noise is from N(0, SD)
3) the SD is constant 

R provides some basic diagnostic plots to assess 2 and 3. Just call `plot` on your model object

```{r}
plot(m1)
```

How to interpret plots:

1. Residuals vs Fitted: should have a horizontal line with uniform and symmetric scatter of points; if not, evidence that SD is not constant

2. Normal Q-Q: points should lie close to diagonal line; if not, evidence that noise is not drawn from N(0, SD)

3. Scale-Location: should have a horizontal line with uniform scatter of point; (similar to #1 but easier to detect trend in dispersion)

4. Residuals vs Leverage: points outside the contour lines are influential observations

By default the 3 "most extreme" points are labeled by row number. 2658 appears in all four plots. It's a really big expensive home.

```{r}
homes[2658,]
```

These plots reveal that our assumptions of normally distributed residuals and constant SD are highly suspect.

What can we do?

Non-constant SD can be evidence of a mispecified model or a very skewed response. Notice that our response is quite skewed:

```{r}
hist(homes$totalvalue)

```


We could try transforming totalvalue to a different scale. A common transformation is a log transformation. This looks a little better.

```{r}
hist(log(homes$totalvalue))
```

Let's try modeling log-transformed totalvalue.

```{r}
m2 <- lm(log(totalvalue) ~ finsqft + bedroom + lotsize, data = homes)
summary(m2)
```

The diagnostic plots look better. 

```{r}
plot(m2)
```

But is this a "good model"? Is our proposed model of weighted sums good? Let's simulate data and compare to observed data.

```{r}
sim2 <- simulate(m2, nsim = 50)
plot(density(log(homes$totalvalue)))
for(i in 1:50)lines(density(sim2[[i]]), lty = 2, col = "grey80")
```

This doesn't look to bad!

Let's say we're happy with this model. How to interpret? The response is log transformed, so interpretation of coefficients changes. We first need to exponentiate and then interpret as multiplicative instead of additive effect. This is one of the drawbacks of log transformation: the complication of interpreting coefficients. Multiplying the coefficients by 100 helps make this easier.

```{r}
round(exp(coef(m2)), 4) 
```


Some naive interpretations:

- each additional finished square foot increases price by 0.0005%. Or divide by 100 to get each additional 100 finished square feet increases price by 0.05%.
- each additional bedroom increases price by about 0.04%
- each additional lot size acre increases price by about 0.004%

All of the p-values refer to hypothesis tests that the coefficients are 0. Many statisticians and researchers don't like these tests and prefer to look at confidence intervals.

```{r}
round(exp(confint(m2)), 4)

```

## YOUR TURN #2

1. Insert a code chunk below and model log(totalvalue) as function of fullbath and finsqft. Call your model `m3`

```{r}
m3 <- lm(log(totalvalue) ~ fullbath + finsqft, data = homes)
summary(m3)
```


2. Insert a code chunk below and check the diagnostic plots

```{r}
plot(m3)
```


3. How do we interpret the fullbath coefficient?

```{r}
round(exp(coef(m3)), 4)
```

4. Insert a code chunk below and simulate data from the model and compare to the observed totalvalue. Does this look like a good model?

```{r}
sim3 <- simulate(m3, nsim = 50)
plot(density(log(homes$totalvalue)))
for(i in 1:50)lines(density(sim3[[i]]), lty = 2, col = "grey80")
```

## Categorical predictors

Let's include `hsdistrict` in our model. Here's a quick tally

```{r}
table(homes$hsdistrict)
```

We can also wrap the `table` function in `proportions` to see the proportions with and without.

```{r}
proportions(table(homes$hsdistrict))
```

These are not numbers, so does R handle this in a linear model? It creates a _contrast_. This is a dummy matrix of zeroes and ones. If you have K levels, you'll have K-1 columns. In this case we'll have two columns: one for Monticello HS and one for Western Albemarle HS. By default R takes whatever level comes first alphabetically and makes it the _baseline_ or _reference_ level. 

- a home in Albemarle HS district gets two zeroes
- a home in Moneticello HS district gets a one in the Monticello column
- a home in Western Albemarle HS district gets a one in the West Alb column

This is called a _treatment contrast_ and is the default in R.

```{r}
contrasts(factor(homes$hsdistrict))
```

Let's go ahead and fit our model. Our proposed model says "high school district has some relationship with total value of home". (Our model may not be right.)

```{r}
m4 <- lm(log(totalvalue) ~ fullbath + finsqft + hsdistrict, data = homes)
summary(m4)
```

The coefficients for Monticello and Western Albemarle are in relation to Albemarle HS. To understand their effect we need to exponentiate.

```{r}
round(exp(coef(m4)), 4)
```

It appears that the value of a home in Western Albemarle will be about 11% higher than an equivalent home in Albemarle. Likewise it appears that the value of a home in the Monticello district will be about 7% less than an equivalent home in the Albemarle district. 

## YOUR TURN #3

1. Insert a code chunk below and model log(totalvalue) as function of fullbath, finsqft and cooling. Call your model `m5`. 

```{r}
m5 <- lm(log(totalvalue) ~ fullbath + finsqft + cooling, data = homes)
summary(m5)
```

2. What is the interpretation of cooling? What is the baseline or reference level?

```{r}
round(exp(coef(m5)), 4)
```

## Modeling Interactions

In our model above that included `hsdistrict` we assumed the effects were _additive_. For example, it didn't matter what high school district your home was in, the effect of `bathroom` or `finsqft` was the same. It also assumed the effect of each additional `fullbath` was the same regardless of how big the house was, and vice versa. This may be too simple. 

Interactions allow the effects of variables to depend on other variables. Again subject matter knowledge helps with the proposal of interactions. As we'll see interactions make your model more flexible but harder to understand.

R makes it really simple to include interactions in models. Just indicate an interaction between two variables by placing a colon (:) between them.

```{r}
m6 <- lm(log(totalvalue) ~ fullbath + finsqft + hsdistrict + 
           fullbath:finsqft + fullbath:hsdistrict + 
           finsqft:hsdistrict, data = homes)
summary(m6)
```

Interpretation is now much more difficult. We cannot directly interpret the _main effects_ of `fullbath`, `finsqft` or `hsdistrict`. Why? Because they interact. What's the effect of `finsqft`? It depends on `fullbath` and `hsdistrict`! 

Effect plots can help us visualize and make sense of models with interactions. Let's make one and talk about what it's showing.

```{r}
library(ggeffects)
plot(ggpredict(m6, terms = c("fullbath", "hsdistrict")))
# place fullbath on x-axis, group by hsdistrict
```

What's the effect of fullbath? It depends. It's more dramatic in Western Albemarle and Monticello. Of course a lot of the difference comes at extreme values of fullbath. 

What exactly was plotted? We can see by calling `ggpredict` without `plot`

```{r}
ggpredict(m6, terms = c("fullbath", "hsdistrict"))
```

`ggpredict` used our model to make totalvalue predictions for various values of `fullbath` in the three school districts, holding finsqft at 1828. 

We can specify our values if we like.

```{r}
plot(ggpredict(m6, terms = c("fullbath[1:5]", "hsdistrict"), 
          condition = c(finsqft = 2000)))
```

What about the effects of `finsqft` and `fullbath`?

```{r}
plot(ggpredict(m6, terms = c("finsqft", "fullbath[1,3,5]"), 
          condition = c(finsqft = 2000)))
```

The effect of `finsqft` seems to taper off the more full baths a house has.